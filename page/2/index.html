<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Page 2 | Hexo</title>

  
  <meta name="author" content="fld">
  

  

  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  

  <meta property="og:site_name" content="Hexo"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
</head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Hexo</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    
  

<article>

  
    
    <h3 class="article-title"><a href="/2014/07/16/2014-07-16-compile-kafka-from-source/"><span>如何从源码编译Kafka0.8.x</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/07/16/2014-07-16-compile-kafka-from-source/" rel="bookmark">
        <time class="entry-date published" datetime="2014-07-15T16:00:00.000Z">
          2014-07-16
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>下载代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone http://git-wip-us.apache.org/repos/asf/kafka.git kafka</div></pre></td></tr></table></figure>
<p>下载可执行文件:<a href="https://archive.apache.org/dist/kafka/0.8.1/kafka_2.9.2-0.8.1.tgz" target="_blank" rel="external">https://archive.apache.org/dist/kafka/0.8.1/kafka_2.9.2-0.8.1.tgz</a></p>
<p>相关文档： <a href="http://kafka.apache.org/code.html" target="_blank" rel="external">http://kafka.apache.org/code.html</a></p>
<p>编译kafka:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">直接运行代码</div><div class="line">./gradlew jar</div></pre></td></tr></table></figure>
<p>如果需要编译某Scala特定版本，则加上版本号 <code>./gradlew -PscalaVersion=2.9.2 jar</code></p>
<p>编译出的jar包在core/build/libs/目录下。</p>
<ul>
<li>性能测试用的jar包在perf/build/libs/目录下，<a href="http://kafka.apache.org" target="_blank" rel="external">官方</a>下载的bin包里面不包含performance相关的包，需要将perf/build/libs/下的kafka-perf_2.9.2-0.8.1.jar拷贝到kafka_2.9.2-0.8.1目录的libs下</li>
</ul>
<p>启动kafka：</p>
<pre><code>#在kafka_2.9.2-0.8.1目录下
#启动zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties &amp;   

#启动kafka server
bin/kafka-server-start.sh config/server.properties &amp;
</code></pre><p>创建topic</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testtopic</div></pre></td></tr></table></figure>
<p>测试Producer性能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bin/kafka-producer-perf-test.sh --broker-list localhost:9092 --topic testtopic --messages 300000 --message-size 1000 --batch-size 200</div></pre></td></tr></table></figure>
<ul>
<li>messages:     总message个数</li>
<li>message-size: message大小</li>
<li>batch-size:     包大小，一个包中message个数</li>
</ul>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Kafka/">Kafka</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/kafka/">kafka</a><a href="/tags/mq/">mq</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/07/14/2014-07-14-centos-install-docker/"><span>CentOS如何安装Docker</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/07/14/2014-07-14-centos-install-docker/" rel="bookmark">
        <time class="entry-date published" datetime="2014-07-13T16:00:00.000Z">
          2014-07-14
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>安装Docker需要CentOS6或以上版本,内核需要2.6.32-431或以上版本。Docker依赖EPEL(<a href="https://fedoraproject.org/wiki/EPEL" target="_blank" rel="external">Extra Packages for Enterprise Linux</a>)，安装EPEL的方法：</p>
<pre><code>#CentOS 5.x

wget http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm
wget http://rpms.famillecollet.com/enterprise/remi-release-5.rpm
sudo rpm -Uvh remi-release-5*.rpm epel-release-5*.rpm


#CentOS 6.x
wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
wget http://rpms.famillecollet.com/enterprise/remi-release-6.rpm
sudo rpm -Uvh remi-release-6*.rpm epel-release-6*.rpm
</code></pre><p>可以在/etc/yum.repos.d目录下，看到多了几个.repo文件：</p>
<pre><code>ls -1 /etc/yum.repos.d/epel* /etc/yum.repos.d/remi.repo
/etc/yum.repos.d/epel.repo
/etc/yum.repos.d/epel-testing.repo
/etc/yum.repos.d/remi.repo
</code></pre><p>开启remi repository，编辑文件remi.repo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo vim /etc/yum.repos.d/remi.repo</div></pre></td></tr></table></figure>
<p>编辑[remi]块中的enabled，设置为1:</p>
<pre><code>[remi]
name=Les RPM de remi pour Enterprise Linux $releasever - $basearch
#baseurl=http://rpms.famillecollet.com/enterprise/$releasever/remi/$basearch/
mirrorlist=http://rpms.famillecollet.com/enterprise/$releasever/remi/mirror
enabled=1  #修改这里的enabled
gpgcheck=1
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-remi
failovermethod=priority
</code></pre><p>安装Docker</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install docker-io</div></pre></td></tr></table></figure>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/docker/">docker</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/docker/">docker</a><a href="/tags/centos/">centos</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/07/11/2014-07-11-git-op/"><span>Git常用技巧</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/07/11/2014-07-11-git-op/" rel="bookmark">
        <time class="entry-date published" datetime="2014-07-10T16:00:00.000Z">
          2014-07-11
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p><strong>最权威的Git书籍 <a href="http://git-scm.com/book/zh" target="_blank" rel="external">http://git-scm.com/book/zh</a></strong></p>
<p>##Git常用技巧</p>
<p>###创建本地项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git init</div><div class="line">git add README.txt</div><div class="line">git commit -m &apos;init project&apos;</div></pre></td></tr></table></figure>
<p>###提交到Remote仓库</p>
<pre><code>git remote add origin https://github.com/wecoders/testproject.git
git push -u origin master   #提交到master分支(默认为主干)
</code></pre><p>这里的origin是可以随便定义的，比如定义为mygithub-project</p>
<pre><code>git remote add mygithub-project https://github.com/wecoders/testproject.git
</code></pre><p>这样远程名mygithub-project就是代表<a href="https://github.com/wecoders/testproject.git" target="_blank" rel="external">https://github.com/wecoders/testproject.git</a></p>
<p>###恢复本地被删除的文件</p>
<p>直接使用checkout指令</p>
<pre><code>git checkout src/test.java
</code></pre><p>或者可以批量恢复被删除的文件</p>
<pre><code>git ls-files -d | xargs git checkout
</code></pre><p>###根据commits生成patch补丁</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">git log | grep ^commit | head -10   #列出最近10条commit</div><div class="line"></div><div class="line">commit 88093868a6a02c0cf5012465a674c360502ffd0d</div><div class="line">commit e7f11a1fd210c70a4fc1ffb66e85c40e6275aa88</div><div class="line">commit 5d754c56cad45a586d1fb44380c760e925f6e9a4</div><div class="line">commit 23a92f158d4829e6f10b193913c28ad43997c6ed</div><div class="line">commit ed7b6a1067a38ff58f47017d18838d97a9f49086</div><div class="line">commit dfedbb074ac30546bc984f5d05d64ab16d0be3d9</div><div class="line">commit 369f714f1916c4d390defa22b86b8971b4b6c00d</div><div class="line">commit 2fa5f2b86709256e75b62bb03c070e9a967a958b</div><div class="line">commit ebe53e01a5b98bd830a5515c110334aa715b2873</div><div class="line">commit 665739e4d7cc439d4198a80852bb19838a32fc1d</div><div class="line"></div><div class="line">git format-patch 665739e4d7cc439d4198a80852bb19838a32fc1d    #根据665739e4...生成patch</div></pre></td></tr></table></figure>
<p>###先checkout某个commit，并据此commit创建分支</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git checkout -b branch_test1 665739e4d7cc439d4198a80852bb19838a32fc1d</div></pre></td></tr></table></figure>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Git/">Git</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/git/">git</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/06/13/2014-06-13-zookeeper-cluster/"><span>Zookeeper集群配置</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/06/13/2014-06-13-zookeeper-cluster/" rel="bookmark">
        <time class="entry-date published" datetime="2014-06-12T16:00:00.000Z">
          2014-06-13
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>安装Zookeeper分为单机、伪集群、集群 三种模式</p>
<p>下载 <a href="http://apache.fayea.com/apache-mirror/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz" target="_blank" rel="external">http://apache.fayea.com/apache-mirror/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</a></p>
<p>解压zookeeper-3.4.6.tar.gz到/home/fld/zookeeper-3.4.6</p>
<p>###伪集群模式</p>
<p>这里假设建立3个节点服务</p>
<pre><code>cd /home/fld/zookeeper-3.4.6
</code></pre><p><strong>创建3个data文件夹:</strong></p>
<pre><code>mkdir data0
mkdir data1
mkdir data2
</code></pre><p><strong>创建3个log文件:</strong></p>
<pre><code>mkdir log0
mkdir log1
mkdir log2
</code></pre><p><strong>创建3个配置文件</strong></p>
<pre><code>cp conf/zoo_sample.cfg conf/zoo_0.cfg
cp conf/zoo_sample.cfg conf/zoo_1.cfg
cp conf/zoo_sample.cfg conf/zoo_2.cfg
</code></pre><p>3个配置文件，除了clientPort有点区别外，其他的配置项都是一样的。</p>
<p><strong>编辑配置文件conf/zoo_0.cfg:</strong></p>
<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/fld/zookeeper-3.4.6/data0
dataLogDir=/home/fld/zookeeper-3.4.6/logs0
clientPort=2181
server.0=127.0.0.1:2887:3887
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
</code></pre><p><strong>编辑配置文件conf/zoo_1.cfg:</strong></p>
<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/fld/zookeeper-3.4.6/data1
dataLogDir=/home/fld/zookeeper-3.4.6/logs1
clientPort=2182
server.0=127.0.0.1:2887:3887
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
</code></pre><p><strong>编辑配置文件conf/zoo_2.cfg:</strong></p>
<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/fld/zookeeper-3.4.6/data2
dataLogDir=/home/fld/zookeeper-3.4.6/logs2
clientPort=2183
server.0=127.0.0.1:2887:3887
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
</code></pre><p><strong>创建myid文件,将相应的id写入myid文件</strong></p>
<pre><code>echo 0 &gt; data0/myid
echo 1 &gt; data1/myid
echo 2 &gt; data2/myid
</code></pre><p><strong>分别启动3个服务</strong></p>
<pre><code>bin/zkServer.sh start conf/zoo_0.cfg
bin/zkServer.sh start conf/zoo_1.cfg
bin/zkServer.sh start conf/zoo_2.cfg
</code></pre><p>###集群模式</p>
<p>集群模式和伪集群模式类似，分别将zookeeper-3.4.6.tar.gz拷贝到3台服务器上，并解压；</p>
<p>创建一个zoo.cfg</p>
<pre><code>cd /home/fld/zookeeper-3.4.6
cp conf/zoo_sample.cfg conf/zoo.cfg
</code></pre><p>3台机器的配置文件zoo.cfg可以保持一样，编辑配置conf/zoo.cfg:</p>
<pre><code>tickTime=2000
initLimit=10
syncLimit=5
dataDir=/home/fld/zookeeper-3.4.6/data
dataLogDir=/home/fld/zookeeper-3.4.6/logs
clientPort=2181
server.0=192.168.0.1:2888:3888
server.1=192.168.0.2:2888:3888
server.2=192.168.0.3:2888:3888
</code></pre><p>分别在3台机器的/home/fld/zookeeper-3.4.6下创建目录data,然后创建myid文件，里面内容填写对应的server id：</p>
<p>机器0:  </p>
<pre><code>mkdir data
echo 0 &gt; data/myid
</code></pre><p>机器1:</p>
<pre><code>mkdir data
echo 1 &gt; data/myid
</code></pre><p>机器2:</p>
<pre><code>mkdir data
echo 2 &gt; data/myid
</code></pre><p>分布在3台机器上启动zookeeper:</p>
<pre><code>/home/fld/zookeeper-3.4.6/bin/zkServer.sh start
</code></pre>
      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/BigData/">BigData</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/zookeeper/">zookeeper</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/05/08/2014-05-08-hbase-book/"><span>HBase Book</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/05/08/2014-05-08-hbase-book/" rel="bookmark">
        <time class="entry-date published" datetime="2014-05-07T16:00:00.000Z">
          2014-05-08
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>#####HDFS指令<br>查看HDFS目录<br>bin/hdfs dfs -ls hdfs://vm01:9000</p>
<p>#####HBase操作指令</p>
<p>启动HBase<br>bin/start-hbase.sh</p>
<p>HBase lib下面的Hadoop相关的jar不是2.3.0，需要替换成2.3.0的jar包<br>rm lib/hadoop*.jar</p>
<p>MAC OS X下<br>find $HADOOP_HOME/share/hadoop/ -name “hadoop*.jar” |grep -v “test” |grep -v “sources.jar” | xargs  -I{} cp {} $HBASE_HOME/lib</p>
<p>Linux下</p>
<p>find $HADOOP_HOME/share/hadoop/ -name “hadoop*.jar” |grep -v “test” |grep -v “sources.jar” | xargs  -i cp {} $HBASE_HOME/lib</p>
<p>create ‘table1’, ‘cf1’</p>
<p>#####Hive操作</p>
<p>  $ tar -xzvf hive-x.y.z.tar.gz<br>  $ cd hive-x.y.z<br>  $ export HIVE_HOME=<br>  $ export PATH=$HIVE_HOME/bin:$PATH</p>
<p>CREATE  EXTERNAL TABLE IF NOT EXISTS <code>history_role_ext</code>(<br>  <code>day</code> string,<br>  <code>grp</code> string,<br>  <code>mac</code> string,<br>  <code>dur</code> double,<br>  <code>role</code> string)<br>PARTITIONED BY (<br>  <code>d</code> string,<br>  <code>g</code> string)<br>ROW FORMAT DELIMITED<br>  FIELDS TERMINATED BY ‘,’<br>STORED AS INPUTFORMAT<br>  ‘org.apache.hadoop.mapred.TextInputFormat’<br>OUTPUTFORMAT<br>  ‘org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat’<br>LOCATION<br>  ‘hdfs://vm01:9000/hive/warehouse/history_role_ext’</p>
<p>load data local inpath ‘/root/input.txt’ overwrite into table history_role_ext partition(d=’20140513’, g=’all’);</p>
<p>CREATE  EXTERNAL TABLE IF NOT EXISTS <code>history_role_external</code>(<br>  <code>day</code> string,<br>  <code>grp</code> string,<br>  <code>mac</code> string,<br>  <code>dur</code> double,<br>  <code>role</code> string)<br>PARTITIONED BY (<br>  <code>d</code> string,<br>  <code>g</code> string)<br>ROW FORMAT DELIMITED<br>  FIELDS TERMINATED BY ‘,’<br>STORED AS TEXTFILE<br>LOCATION<br>  ‘hdfs://vm01:9000/hive/warehouse/history_role_external’</p>
<p>load data local inpath ‘/root/input.txt’ overwrite into table history_role_external partition(d=’20140513’, g=’all’);</p>
<p>#####Hadoop操作<br>配置Single Node方法 <a href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide" target="_blank" rel="external">http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide</a></p>
<p><a href="http://codesfusion.blogspot.jp/2013/10/hadoop-wordcount-with-new-map-reduce-api.html" target="_blank" rel="external">http://codesfusion.blogspot.jp/2013/10/hadoop-wordcount-with-new-map-reduce-api.html</a></p>
<h6 id="Start-HDFS-daemons"><a href="#Start-HDFS-daemons" class="headerlink" title="Start HDFS daemons"></a>Start HDFS daemons</h6><p>$HADOOP_PREFIX/bin/hdfs namenode -format</p>
<h6 id="Start-the-namenode-daemon"><a href="#Start-the-namenode-daemon" class="headerlink" title="Start the namenode daemon"></a>Start the namenode daemon</h6><p>$HADOOP_PREFIX/sbin/hadoop-daemon.sh start namenode</p>
<h6 id="Start-the-datanode-daemon"><a href="#Start-the-datanode-daemon" class="headerlink" title="Start the datanode daemon"></a>Start the datanode daemon</h6><p>$HADOOP_PREFIX/sbin/hadoop-daemon.sh start datanode</p>
<h6 id="Start-YARN-daemons"><a href="#Start-YARN-daemons" class="headerlink" title="Start YARN daemons"></a>Start YARN daemons</h6><h6 id="Start-the-resourcemanager-daemon"><a href="#Start-the-resourcemanager-daemon" class="headerlink" title="Start the resourcemanager daemon"></a>Start the resourcemanager daemon</h6><p>$HADOOP_PREFIX/sbin/yarn-daemon.sh start resourcemanager</p>
<h6 id="Start-the-nodemanager-daemon"><a href="#Start-the-nodemanager-daemon" class="headerlink" title="Start the nodemanager daemon"></a>Start the nodemanager daemon</h6><p>$HADOOP_PREFIX/sbin/yarn-daemon.sh start nodemanager</p>
<p><a href="http://wenku.baidu.com/view/d282172055270722192ef7ba.html" target="_blank" rel="external">http://wenku.baidu.com/view/d282172055270722192ef7ba.html</a></p>
<p>#####Git操作</p>
<p><a href="http://www.infoq.com/cn/news/2011/03/git-adventures-branch-merge" target="_blank" rel="external">http://www.infoq.com/cn/news/2011/03/git-adventures-branch-merge</a></p>
<p>Hive性能调优</p>
<p>1、数据存TextFile，查询性能比较慢，使用ORCFile，速度快<br>2、加载数据，先加载TextFile到table1,再重table1加载到table2, table2采用ORCFile格式存储<br>设置reduce job个数, set mapreduce.job.reduces=6;</p>
<p>MapReduce<br>1、出现Type错误，大部分错误原因是 job.setOutputValueClass(LongWritable.class); 设置不正确。这里的OutputValueClass不是Reduce的Output，而是Map的Output。</p>
<p>CREATE  EXTERNAL TABLE IF NOT EXISTS <code>history_role</code>(<br>  <code>day</code> string,<br>  <code>grp</code> int,<br>  <code>mac</code> string,<br>  <code>st</code> string,<br>  <code>et</code> string,<br>  <code>dur</code> int,<br>  <code>role</code> int,<br>  <code>fac</code> string)<br>PARTITIONED BY (<br>  <code>d</code> string)<br>ROW FORMAT DELIMITED<br>  FIELDS TERMINATED BY ‘,’<br>STORED AS TEXTFILE<br>LOCATION ‘hdfs://vm01:9000/hive/warehouse/history_role’;</p>
<p>CREATE  EXTERNAL TABLE IF NOT EXISTS <code>history_role</code>(<br>  <code>day</code> string,<br>  <code>grp</code> int,<br>  <code>mac</code> string,<br>  <code>st</code> string,<br>  <code>et</code> string,<br>  <code>dur</code> int,<br>  <code>role</code> int,<br>  <code>fac</code> string)<br>PARTITIONED BY (<br>  <code>d</code> string)<br>ROW FORMAT DELIMITED<br>  FIELDS TERMINATED BY ‘,’<br>STORED AS TEXTFILE<br>LOCATION ‘hdfs://localhost/hive/warehouse/history_role’;</p>
<p>CREATE  EXTERNAL TABLE IF NOT EXISTS <code>history_role</code>(<br>  <code>day</code> string,<br>  <code>grp</code> int,<br>  <code>mac</code> string,<br>  <code>st</code> string,<br>  <code>et</code> string,<br>  <code>dur</code> int,<br>  <code>role</code> int,<br>  <code>fac</code> string)<br>PARTITIONED BY (<br>  <code>d</code> string)<br>CLUSTERED BY (day)<br>SORTED BY (grp)<br>INTO 8 BUCKETS<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’<br>STORED AS orc tblproperties (“orc.compress”=”NONE”, “orc.row.index.stride”=”10000”, “orc.stripe.size”=”10240000”)</p>
<p>设置分区<br>set hive.exec.dynamic.partition.mode=nonstrict;</p>
<p>设置reduce个数<br>set mapreduce.job.reduces=8;</p>
<p>set set hive.enforce.bucketing=true;<br>set set hive.enforce.sorting=true;</p>
<p>######从history_role1中加载数据到history_role2</p>
<p>from history_role_text<br>insert overwrite table history_role<br>partition(d)<br>select day ,grp, mac, dur, role, day, d as d where day = ‘20140515’;</p>
<p>######load data from local path</p>
<p>load data local inpath ‘/root/part-00000’ overwrite into table history_role1 partition(d=’20140515’);</p>
<p>hdfs dfs -ls hdfs://vm01:9000/hive/warehouse/history_role</p>
<p>load data local inpath ‘/usr/java/data/role/2014-06-13/part-00000’ overwrite into table history_role partition(d=’2014-06-13’);</p>
<p>LOAD DATA INFILE ‘/root/role.txt’ INTO TABLE <code>historyrole</code><br>FIELDS TERMINATED BY ‘,’<br>OPTIONALLY ENCLOSED BY ‘“‘<br>ESCAPED BY ‘’<br>LINES TERMINATED BY ‘\n’<br>(day, grp, mac, st, et, dur, role, fac)</p>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/bigdata/">bigdata</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Hadoop/">Hadoop</a><a href="/tags/HBase/">HBase</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/04/16/2014-04-16-hadoop-hbase-cdh-impala/"><span>安装CDH</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/04/16/2014-04-16-hadoop-hbase-cdh-impala/" rel="bookmark">
        <time class="entry-date published" datetime="2014-04-15T16:00:00.000Z">
          2014-04-16
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>CentOS</p>
<p>#配置Hadoop</p>
<p>假设配置3台服务器：</p>
<p>192.168.100.1 node1     (master)   </p>
<p>192.168.100.2 node2 (slave1)</p>
<p>192.168.100.3 node3 (slave2)</p>
<p>在/etc/hosts中添加机器名配置：(将下面内容添加到/etc/hosts中)</p>
<pre><code>192.168.100.1 node1
192.168.100.2 node2
192.168.100.3 node3
</code></pre><p>###创建用户</p>
<pre><code>$ useradd hadoop
$ cd /home/hadoop
</code></pre><p>###配置NameNode(node1)的ssh无需密码登录</p>
<p>#####安装SSH：</p>
<p>sudo apt-get install ssh<br>或<br>yum install ssh</p>
<p>#####SSH无密码登录</p>
<p>使用root编辑 <strong>/etc/ssh/sshd_config</strong> 将下面3个配置前面的注释去掉</p>
<pre><code>RSAAuthentication yes

PubkeyAuthentication yes

AuthorizedKeysFile    .ssh/authorized_keys
</code></pre><p>然后重启ssh服务: service sshd restart<br>3台机器都需要修改</p>
<p>####创建id_rsa</p>
<pre><code>$ ssh-keygen -t rsa
生成密钥对，会询问密码，直接回车采用默认路径，直到结束
$ cd /home/hadoop/.ssh
$ cp id_rsa.pub authorized_keys
</code></pre><p>测试</p>
<pre><code>$ ssh localhost

$ ssh node1
</code></pre><p>第一次ssh会有提示信息：</p>
<p>The authenticity of host ‘node1 (192.168.100.1)’ can’t be established.<br>RSA key fingerprint is 03:e0:30:cb:6e:13:a8:70:c9:7e:cf:ff:33:2a:67:30.<br>Are you sure you want to continue connecting (yes/no)?</p>
<p>输入yes</p>
<p>如果不需要密码，则连接成功</p>
<p>在node2和node3上，分别执行:</p>
<pre><code>$ su hadoop
$ cd /home/hadoop
$ ssh-keygen -t rsa
</code></pre><p>回到node1上:</p>
<pre><code>$ scp authorized_keys   node2:/home/hadoop/.ssh/
$ scp authorized_keys   node3:/home/hadoop/.ssh/
</code></pre><p>或:</p>
<pre><code>$ cat id_rsa.pub | ssh hadoop@node2 &apos;cat - &gt;&gt; ~/.ssh/authorized_keys&apos;
$ cat id_rsa.pub | ssh hadoop@node3 &apos;cat - &gt;&gt; ~/.ssh/authorized_keys&apos;
</code></pre><p>这里需要输入node2和node3密码</p>
<p>测试是否成功：</p>
<pre><code>$ ssh node2
$ ssh node3
</code></pre><p>######在CentOS中，如果还是需要输入密码，可以执行下面的命令：（否则权限太大，CentOS不让通过？）</p>
<pre><code>chmod 600 .ssh/authorized_keys
chmod 700 .ssh
</code></pre>
      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/bigdata/">bigdata</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Hadoop/">Hadoop</a><a href="/tags/HBase/">HBase</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/04/15/2014-04-15-how-to-nginx-to-static-files-server/"><span>Nginx配置静态文件服务器</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/04/15/2014-04-15-how-to-nginx-to-static-files-server/" rel="bookmark">
        <time class="entry-date published" datetime="2014-04-14T16:00:00.000Z">
          2014-04-15
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>##Nginx配置静态文件服务器(转)</p>
<p>该配置可以轻松支撑每分钟上千的请求，并用一些安全方面的设置</p>
<p>配置文件 nginx.conf</p>
<hr>
<pre><code>#worker进程的数量
worker_processes  3;

#worker进程可以打开的最大文件句柄数
#worker_rlimit_nofile 1024;

events {
    worker_connections  64;
}

http {

 ## Size Limits
 #
 #client_body_buffer_size   8k;
 #client_header_buffer_size 1k;
 #client_max_body_size      1m;
 #large_client_header_buffers 4 4k/8k;

 ## Timeouts
 #client_body_timeout     60;
 #client_header_timeout   60;
  keepalive_timeout       300 300;
 #send_timeout            60;

 ## General Options
  charset                 utf-8;
  default_type            application/octet-stream;
  ignore_invalid_headers  on;
  include                 /etc/mime.types;
  keepalive_requests      20;
 #keepalive_disable       msie6;
  max_ranges              0;
 #open_file_cache         max=1000 inactive=1h;
 #open_file_cache_errors  on;
 #open_file_cache_min_uses 3;
 #open_file_cache_valid   1m;
  recursive_error_pages   on;
  sendfile                on;
  server_tokens           off;
 #server_name_in_redirect on;
  source_charset          utf-8;
 #tcp_nodelay             on;
 #tcp_nopush              off;

 ## Request limits
  limit_req_zone  $binary_remote_addr  zone=gulag:1m   rate=60r/m;

 ## Compression
  gzip              on;
  gzip_static       on;
 #gzip_buffers      16 8k;
 #gzip_comp_level   1;
 #gzip_http_version 1.0;
 #gzip_min_length   0;
 #gzip_types        text/plain text/html text/css image/x-icon image/bmp;
  gzip_vary         on;

 ## Log Format
  log_format  main  &apos;$remote_addr $host $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; $ssl_cipher $request_time&apos;;

 ## Deny access to any host other than (www.)mydomain.com. Only use this
 ## option is you want to lock down the name in the Host header the client sends.
  # server {
  #      server_name  _;  #default
  #      return 444;
  #  }

 ## Server (www.)mydomain.com
  server {
      add_header  Cache-Control public;
      access_log  /var/log/nginx/access.log main buffer=32k;
      error_log   /var/log/nginx/error.log error;
      expires     max;
      limit_req   zone=gulag burst=200 nodelay;
      listen      127.0.0.1:80;
      root        /htdocs;
      server_name mydomain.com www.mydomain.com;

     ## Note: if{} sections are expensive to process. Please only use them if you need them
     ## and take a look lower down on the page for our discussion of if{} statements.

     ## Only allow GET and HEAD request methods. By default Nginx blocks
     ## all requests type other then GET and HEAD for static content.
     # if ($request_method !~ ^(GET|HEAD)$ ) {
     #   return 405;
     # }

     ## Deny illegal Host headers.
     # if ($host !~* ^(mydomain.com|www.mydomain.com)$ ) {
     #  return 405;
     # }

     ## Deny certain User-Agents (case insensitive)
     ## The ~* makes it case insensitive as opposed to just a ~
     # if ($http_user_agent ~* (Baiduspider|Jullo) ) {
     #  return 405;
     # }

     ## Deny certain Referers (case insensitive)
     ## The ~* makes it case insensitive as opposed to just a ~
     # if ($http_referer ~* (babes|click|diamond|forsale|girl|jewelry|love|nudit|organic|poker|porn|poweroversoftware|sex|teen|video|webcam|zippo) ) {
     #  return 405;
     # }

     ## Redirect from www to non-www
     # if ($host = &apos;www.mydomain.com&apos; ) {
     #  rewrite  ^/(.*)$  http://mydomain.com/$1  permanent;
     # }

     ## Stop Image and Document Hijacking
     #location ~* (\.jpg|\.png|\.css)$ {
     #   if ($http_referer !~ ^(http://mydomain.com) ) {
     #     return 405;
     #   }
     # }

     ## Restricted Access directory by password in the access_list file.
      location ^~ /secure/ {
            allow 127.0.0.1/32;

            deny all;
            auth_basic &quot;RESTRICTED ACCESS&quot;;
            auth_basic_user_file /var/www/htdocs/secure/access_list;
        }

     ## Only allow these full URI paths relative to document root. If you only want
     ## to reference the file name use $request_filename instead of $request_uri. By default
     ## nginx will only serve out files in &quot;root /htdocs;&quot; defined above so this block is not needed, just an example.
     #  if ($request_uri ~* (^\/|\.html|\.jpg|\.org|\.png|\.css|favicon\.ico|robots\.txt)$ ) {
     #    break;
     #  }
     #  return 405;

     ## Serve an empty 1x1 gif _OR_ an error 204 (No Content) for favicon.ico
      location = /favicon.ico {
       #empty_gif;
        return 204;
      }

      ## System Maintenance (Service Unavailable)
      if (-f $document_root/system_maintenance.html ) {
        error_page 503 /system_maintenance.html;
        return 503;
      }

     ## All other errors get the generic error page
      error_page 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 495 496 497
                 500 501 502 503 504 505 506 507 /error_page.html;
      location  /error_page.html {
          internal;
      }
  }
}
</code></pre><hr>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/nginx/">nginx</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/nginx/">nginx</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/02/28/2014-02-28-java-spring-errors/"><span>Java项目中出现的各种错误的解决办法</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/02/28/2014-02-28-java-spring-errors/" rel="bookmark">
        <time class="entry-date published" datetime="2014-02-27T16:00:00.000Z">
          2014-02-28
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>环境：Tomcat6.0</p>
<p>######告警：INFO: validateJarFile(servlet-api.jar) - jar not loaded See Servlet Spec 2.3, section 9.7.2. Offending class: javax/servlet/Servlet.class</p>
<p>解决：把/WEB-INF/lib/下的servlet-api.jar删除掉，Tomcat里面已经包含了servlet-api.jar了，重复。</p>
<p>环境：Hibernate 3.x</p>
<p>######错误：Caused by: java.lang.NoClassDefFoundError: org/hibernate/annotations/Entity</p>
<p>解决：在Hibernate3.5以下，使用注解还需要添加hibernate-annotations-3.3.0.jar，hibernate-commons-annotations.jar和ejb3-persistence.jar等，Hibernate3.5版本已经包含了JPA相关的jar，所以直接导入hibernate3（3.5）.jar即可</p>
<p>环境：Spring3.0.5 tiles2.2.1</p>
<p>######错误：Caused by: java.lang.ClassNotFoundException: org.apache.tiles.startup.BasicTilesInitializer</p>
<p>解决：原因是tiles-core2.2.1中该类BasicTilesInitializer已经deprecated了<br>使用如下配置：</p>
<pre><code>&lt;bean id=&quot;viewResolver&quot;
    class=&quot;org.springframework.web.servlet.view.UrlBasedViewResolver&quot;&gt;
    &lt;property name=&quot;viewClass&quot;&gt;
        &lt;value&gt;
            org.springframework.web.servlet.view.tiles2.TilesView
        &lt;/value&gt;
    &lt;/property&gt;
&lt;/bean&gt;
&lt;bean id=&quot;tilesConfigurer&quot; class=&quot;org.springframework.web.servlet.view.tiles2.TilesConfigurer&quot;&gt;
    &lt;property name=&quot;definitions&quot;&gt;
        &lt;list&gt;
            &lt;value&gt;/WEB-INF/tiles-def.xml&lt;/value&gt;
        &lt;/list&gt;
    &lt;/property&gt;
    &lt;property name=&quot;preparerFactoryClass&quot; value=&quot;org.springframework.web.servlet.view.tiles2.SpringBeanPreparerFactory&quot; /&gt;
&lt;/bean&gt;
</code></pre><p>环境：SpringJUnit4ClassRunner.class</p>
<p>######错误：IOException parsing XML document from class path resource [applicationContext.xml]; 使用classpath:applicationContext.xml找不到XML文件</p>
<p>解决：把classpath换成file<br>    file:WebRoot/WEB-INF/applicationContext.xml</p>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/Java/">Java</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Hibernate/">Hibernate</a><a href="/tags/Spring/">Spring</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/02/09/2014-02-09-storm-learn/"><span>Storm学习笔记</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/02/09/2014-02-09-storm-learn/" rel="bookmark">
        <time class="entry-date published" datetime="2014-02-08T16:00:00.000Z">
          2014-02-09
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>#Storm</p>
<p>牛逼的产品就是使用起来简单，而自身内含不简单，Storm就是之一。</p>
<p><a href="http://qing.blog.sina.com.cn/2294942122/88ca09aa33002dsh.html?sudaref=www.google.com" target="_blank" rel="external">流处理框架Storm简介</a></p>
<p><a href="http://www.searchtb.com/2012/09/introduction-to-storm.html" target="_blank" rel="external">Storm简介</a></p>
<p>这些文章写的非常好，做一些学习笔记。</p>
<p>###介绍</p>
<p>分主从2种节点，3种不同的Daemon:Nimbus运行在主节点上, 从节点上运行Supervisor，每个从节点上还有一系列的worker process来运行具体任务。Daemon之间的信息交换统统是通过Zookeeper来实现。</p>
<p>Nimbus，主要负责接收客户端提交的Topology，进行相应的验证，分配任务，进而把任务相关的元信息写入Zookeeper相应目录，还负责通过Zookeeper来监控任务执行情况；</p>
<p>Supervisor，负责监听Nimbus分配的任务，根据实际情况启动/停止工作进程(Worker)；</p>
<p>Worker，运行具体处理组件逻辑的进程；</p>
<p>过程涉及到了3个相关实体：</p>
<ol>
<li><p>Worker：一个完整的Topology是由分布在多个节点上的Worker进程来执行的，每个Worker都执行（且仅执行）Topology的一个子集。</p>
</li>
<li><p>Executor：在每个Worker内部，会有多个Executor，每个executor对应一个线程。</p>
</li>
<li><p>Task：执行具体数据处理的相关实体，也就是用户实现的Spout/Blot实例。Storm中，一个executor可能会对应一个或者多个task。这就是说，系统中executor的数量是小于等于task的数量的。</p>
</li>
</ol>
<p>Storm和Hadoop的对比:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Hadoop</th>
<th style="text-align:center">Storm</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">JobTracker</td>
<td style="text-align:center">Nimbus</td>
<td></td>
</tr>
<tr>
<td style="text-align:center">TaskTracker</td>
<td style="text-align:center">Supervisor</td>
</tr>
<tr>
<td style="text-align:center">Child</td>
<td style="text-align:center">Worker</td>
</tr>
<tr>
<td style="text-align:center">Job</td>
<td style="text-align:center">Topology</td>
</tr>
<tr>
<td style="text-align:center">Mapper/Reducer</td>
<td style="text-align:center">Spout/Bolt</td>
</tr>
</tbody>
</table>
<ol>
<li><p>Topology：Storm中运行的一个实时应用程序，因为各个组件间的消息流动形成逻辑上的一个拓扑结构。</p>
</li>
<li><p>Spout：在一个Topology中产生源数据流的组件。通常情况下Spout会从外部数据源中读取数据，然后转换为Topology内部的源数据。Spout是一个主动的角色，其接口中有个nextTuple()函数，Storm框架会不停地调用此函数，用户只要在其中生成源数据即可。</p>
</li>
<li><p>Bolt：在一个Topology中接受数据然后执行处理的组件。Bolt可以执行过滤、函数操作、合并、写数据库等任何操作。Bolt是一个被动的角色，其接口中有个execute(Tuple input)函数,在接受到消息后会调用此函数，用户可以在其中执行自己想要的操作。</p>
</li>
<li><p>Tuple：一次消息传递的基本单元。本来应该是一个Key-Value的map，但是由于各个组件间传递的Tuple的字段名称已经事先定义好，所以Tuple中只要按序填入各个value就行了，所以就是一个value list.</p>
</li>
<li><p>Stream：源源不断传递的tuple就组成了Stream。</p>
</li>
<li><p>Stream Grouping: Storm中提供若干种实用的grouping方式，包括shuffle, fields hash, all, global, none, direct和localOrShuffle等。</p>
</li>
</ol>
<p>Storm记录级容错的基本原理和事务拓扑可以参考文前的链接文章。</p>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/bigdata/">bigdata</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Storm/">Storm</a>
    </span>
    

    </div>

    
  </div> -->
</article>






  

<article>

  
    
    <h3 class="article-title"><a href="/2014/01/01/2014-01-01-mysql-op/"><span>MySQL常见操作（不断更新）</span></a></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2014/01/01/2014-01-01-mysql-op/" rel="bookmark">
        <time class="entry-date published" datetime="2013-12-31T16:00:00.000Z">
          2014-01-01
        </time>
      </a>
    </span>
  </div>


  <!-- 

  <div class="article-content">
    <div class="entry">
      
        <p>MySQL创建GBK字符集数据库：</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">GBK: create database `test` DEFAULT CHARACTER SET gbk COLLATE gbk_chinese_ci;</div><div class="line"></div><div class="line">UTF8: CREATE DATABASE `test` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;</div></pre></td></tr></table></figure>
</code></pre><p>MySQL修改root密码</p>
<ul>
<li><p>用SET PASSWORD命令</p>
<p>  <code>SET PASSWORD FOR &#39;root&#39;@&#39;localhost&#39; = PASSWORD(&#39;mynewpassword&#39;);</code></p>
</li>
<li><p>用mysqladmin</p>
<p>  <code>mysqladmin -u root password &quot;mynewpassword&quot;</code></p>
</li>
</ul>
<pre><code>如果root已经设置过密码，采用如下方法

`mysqladmin -u root password oldpass &quot;mynewpassword&quot;`
</code></pre><ul>
<li><p>用UPDATE直接编辑user表</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mysql&gt; use mysql;</div><div class="line"></div><div class="line">mysql&gt; UPDATE user SET Password = PASSWORD(&apos;mynewpassword&apos;) WHERE user = &apos;root&apos;;</div><div class="line"></div><div class="line">mysql&gt; FLUSH PRIVILEGES;</div></pre></td></tr></table></figure>
</li>
<li><p>如果root密码丢失，可以这样：<br>  使用下面的方法，重启mysql</p>
<p>  <code>mysqld_safe --skip-grant-tables&amp;</code></p>
<p>  然后登录mysql</p>
<pre><code>mysql -u root mysql    
mysql&gt; UPDATE user SET password=PASSWORD(&quot;mynewpassword&quot;) WHERE user=&apos;root&apos;;
mysql&gt; FLUSH PRIVILEGES;
</code></pre></li>
</ul>
<p>MySQL中取Group By后的Top N问题<br>比如取group by后每个分组的Top 3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">SELECT</div><div class="line">    mac_group,</div><div class="line">    mac,</div><div class="line">    dur</div><div class="line">FROM</div><div class="line">(</div><div class="line">    SELECT</div><div class="line">        mac_group,</div><div class="line">        sum(sum_dur) as dur,</div><div class="line">        mac,</div><div class="line">        @rn := IF(@prev = mac_group and @prev_mac = mac, @rn + 1, 1) AS rn,</div><div class="line">        @prev := mac_group,</div><div class="line">        @prev_mac := mac</div><div class="line">    FROM mac_customer_stats</div><div class="line">    JOIN (SELECT @prev := NULL, @prev_mac :=NULL, @rn := 0) AS vars</div><div class="line">    where mac_group in (1,2,3)</div><div class="line">    group by mac_group, mac, day</div><div class="line">    ORDER BY mac_group, mac, dur DESC</div><div class="line">) AS T1</div><div class="line">WHERE rn &lt;= 3;</div></pre></td></tr></table></figure>

      
    </div>

  </div> -->

  <!-- <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/MySQL/">MySQL</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/mysql/">mysql</a>
    </span>
    

    </div>

    
  </div> -->
</article>







<nav class="pagination">
  
  <a href="/" class="pagination-prev">Prev</a>
  
  
  <a href="/page/3/" class="pagination-next">Next</a>
  
</nav>

    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2017 fld
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>